{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNJORQhJG0KwXHoCtyoK/Qe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxWMvs_ppc93","executionInfo":{"status":"ok","timestamp":1752340997256,"user_tz":-540,"elapsed":194201,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"462f8d21-6ac3-4375-b3d1-4835100f6362"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch==2.1.0\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.16.0\n","  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio==2.1.0\n","  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.14.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n","Collecting triton==2.1.0 (from torch==2.1.0)\n","  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2025.7.9)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n","Installing collected packages: triton, torch, torchvision, torchaudio\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.6.0+cu124\n","    Uninstalling torchaudio-2.6.0+cu124:\n","      Successfully uninstalled torchaudio-2.6.0+cu124\n","Successfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118 triton-2.1.0\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Collecting pytorch-lightning\n","  Using cached pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (3.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Collecting fire\n","  Using cached fire-0.7.0.tar.gz (87 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.1.0+cu118)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.14.1)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n","Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=5705dfab444fde35f5d2f9f82ed73bb19fa485799673e2df2f6c94ee2f673054\n","  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n","Successfully built fire\n","Installing collected packages: lightning-utilities, fire, torchmetrics, pytorch-lightning\n","Successfully installed fire-0.7.0 lightning-utilities-0.14.3 pytorch-lightning-2.5.2 torchmetrics-1.7.4\n","Collecting lightning\n","  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n","Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.2)\n","Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.14.3)\n","Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\n","Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.1.0+cu118)\n","Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.7.4)\n","Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.14.1)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.11.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.1.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n","Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning\n","Successfully installed lightning-2.5.2\n"]}],"source":["# Cell 1: Fix NumPy version and install dependencies\n","!pip install numpy==1.26.4\n","!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n","!pip install tensorboard pytorch-lightning termcolor tqdm fire Pillow\n","!pip install lightning"]},{"cell_type":"code","source":["# Cell 2: Download dataset\n","import os\n","if not os.path.exists('data'):\n","    !wget https://utexas.box.com/shared/static/qubjm5isldqvyimfj9rsmbnvnbezwcv4.zip -O supertux_data.zip\n","    !unzip -q supertux_data.zip\n","    print(\"✅ Dataset downloaded and extracted\")\n","else:\n","    print(\"✅ Dataset already exists\")\n","\n","# Create directories\n","!mkdir -p homework\n","!mkdir -p logs\n","!mkdir -p checkpoints"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLrRLSUwpi6P","executionInfo":{"status":"ok","timestamp":1752341011455,"user_tz":-540,"elapsed":14193,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"d38d40db-40ea-48c0-b01f-ed5867d557ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-07-12 17:23:17--  https://utexas.box.com/shared/static/qubjm5isldqvyimfj9rsmbnvnbezwcv4.zip\n","Resolving utexas.box.com (utexas.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n","Connecting to utexas.box.com (utexas.box.com)|74.112.186.157|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/qubjm5isldqvyimfj9rsmbnvnbezwcv4.zip [following]\n","--2025-07-12 17:23:17--  https://utexas.box.com/public/static/qubjm5isldqvyimfj9rsmbnvnbezwcv4.zip\n","Reusing existing connection to utexas.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://utexas.app.box.com/public/static/qubjm5isldqvyimfj9rsmbnvnbezwcv4.zip [following]\n","--2025-07-12 17:23:17--  https://utexas.app.box.com/public/static/qubjm5isldqvyimfj9rsmbnvnbezwcv4.zip\n","Resolving utexas.app.box.com (utexas.app.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n","Connecting to utexas.app.box.com (utexas.app.box.com)|74.112.186.157|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!bxkk5u2PiHv0kuQdV5D5OBRNXySJcOAznGZdAbCdieHJ9bGpug3O3_ENiqBDC41pRNcZB_GQyWcveQh3sLwn8vvtmy9Dyhi4F5x0vaEFt1Hv_R1FuRdRkLVC64o3q4MycXzRar2e0y7aTNdD-9Yh3j1UjQGg_ReBOmOMVVG-j8dsvA7P8jxLb7usPpC6uBztd7dMrRizBdpKsualSj1Xg54NrgYgIATKJc1Wn4bGpRAwBJ24jbMD9xm8pcnraeic1EKnlBHcZVNvYfI4d2uieQpznsBmuXH9567d43FPAfej3JSiOSp8_vDNB_9oM_9zdMcFER066gG6vXLTIBxBi7ElrsNevitlnVMLshSG7lU44rkt620fwobwPf0rcOs8-YUlUix0aPMmdPJOqh2xRWSTm65LHA_BhqNV5wKnYO4EP77in2RhJ_hEmIEnYsK80QqkLsOJaOvUdQPGCs2fctKTs5KaqX9DeQt3aEnG7s5GiyvSVjEfYRVYZxPYgHLIpPGBk13ZkE605z4IBApjGEEs3yHTR4wAy0M0JY9a6pG31BBxvgEs804jVZUuKM36obaRd_zG6QBw0ediD__JbyF4gvX99xygsOPWjhYt1O8RyFAWDI7oRh6nD7ln1J3zOXY90k3tTYE50AkPpmx7U5tDNMvPlXZKvYVnwbx-365cBVHaG-ifknE7-L_b0R04ZoBu06hYOXHfk4QbFtXybIVIgUHolgiP6oR4fgkxBdEBcTU_27QiyIj0m9P2F3YnxJDL6MsQUKR5wV-qGVK3Xpz05gqUTnbq3XwnXcCzdLib9_4C4l5u4CB48Abb0wkKX-zvNKb7ocwAjuZqT0VHxxNSEUgXKFNaGloJfHDQbesXx9sm4nPL4lXlZmsWbA9rF0O8tbNYDiXsLLjqzWMaYDy9xNeTCp38sEX875vu0rik7Pc0RUHk71U4w3ftUhjPTY_YxFERRv1qkiGEBOko6FXOT-lriXRQ0yLcoumY3l4pimlOtEPrEoVcsbPcS36RWDN12JnmBn6VEVdZCp3HU5cOWMyCaC_dbMDsTFYFLdJS8829gZxBC-8ZJ1lciVl98GlxVBKOe6hIMCHAzUpDicBdyRwuBcq61Yu-QTlmWcC6PNBgLSEWoi567H4H7X9RrBNeh4trhMjgXHJBBHjeu1p5vB34RmWSqxZuu_Rrga0F7QHnocmrEBRJB7HoD5K5n9OQiPK8rL3ycRhsFiEdiA9MjRsoFsJN7V7kNK0HzJxsT_1Mxg1ANr3Z6mP_1sdYegu0FLQEuHvnVpUOJrpw0dNW7sXMGOK1d-ZI3ae-I-c-9f4XoZ4dCKJ6qoZJ8_OgKFzXW3lNETWsRqla8F7iRO7XSL_QzmiYUz8KUzuiFj17kty1yI2pN2f1Q36rjFSjkABj_r9xNfevEGIranwRuq1GIYGItyaqHW2Y3b_Iwf0EjmvAfQO3jdORTUMTMKpEBLkOH80Ha5KVPcG1gHJhXliwIqD5V_zqNuQL3WxPkcIx/download [following]\n","--2025-07-12 17:23:18--  https://public.boxcloud.com/d/1/b1!bxkk5u2PiHv0kuQdV5D5OBRNXySJcOAznGZdAbCdieHJ9bGpug3O3_ENiqBDC41pRNcZB_GQyWcveQh3sLwn8vvtmy9Dyhi4F5x0vaEFt1Hv_R1FuRdRkLVC64o3q4MycXzRar2e0y7aTNdD-9Yh3j1UjQGg_ReBOmOMVVG-j8dsvA7P8jxLb7usPpC6uBztd7dMrRizBdpKsualSj1Xg54NrgYgIATKJc1Wn4bGpRAwBJ24jbMD9xm8pcnraeic1EKnlBHcZVNvYfI4d2uieQpznsBmuXH9567d43FPAfej3JSiOSp8_vDNB_9oM_9zdMcFER066gG6vXLTIBxBi7ElrsNevitlnVMLshSG7lU44rkt620fwobwPf0rcOs8-YUlUix0aPMmdPJOqh2xRWSTm65LHA_BhqNV5wKnYO4EP77in2RhJ_hEmIEnYsK80QqkLsOJaOvUdQPGCs2fctKTs5KaqX9DeQt3aEnG7s5GiyvSVjEfYRVYZxPYgHLIpPGBk13ZkE605z4IBApjGEEs3yHTR4wAy0M0JY9a6pG31BBxvgEs804jVZUuKM36obaRd_zG6QBw0ediD__JbyF4gvX99xygsOPWjhYt1O8RyFAWDI7oRh6nD7ln1J3zOXY90k3tTYE50AkPpmx7U5tDNMvPlXZKvYVnwbx-365cBVHaG-ifknE7-L_b0R04ZoBu06hYOXHfk4QbFtXybIVIgUHolgiP6oR4fgkxBdEBcTU_27QiyIj0m9P2F3YnxJDL6MsQUKR5wV-qGVK3Xpz05gqUTnbq3XwnXcCzdLib9_4C4l5u4CB48Abb0wkKX-zvNKb7ocwAjuZqT0VHxxNSEUgXKFNaGloJfHDQbesXx9sm4nPL4lXlZmsWbA9rF0O8tbNYDiXsLLjqzWMaYDy9xNeTCp38sEX875vu0rik7Pc0RUHk71U4w3ftUhjPTY_YxFERRv1qkiGEBOko6FXOT-lriXRQ0yLcoumY3l4pimlOtEPrEoVcsbPcS36RWDN12JnmBn6VEVdZCp3HU5cOWMyCaC_dbMDsTFYFLdJS8829gZxBC-8ZJ1lciVl98GlxVBKOe6hIMCHAzUpDicBdyRwuBcq61Yu-QTlmWcC6PNBgLSEWoi567H4H7X9RrBNeh4trhMjgXHJBBHjeu1p5vB34RmWSqxZuu_Rrga0F7QHnocmrEBRJB7HoD5K5n9OQiPK8rL3ycRhsFiEdiA9MjRsoFsJN7V7kNK0HzJxsT_1Mxg1ANr3Z6mP_1sdYegu0FLQEuHvnVpUOJrpw0dNW7sXMGOK1d-ZI3ae-I-c-9f4XoZ4dCKJ6qoZJ8_OgKFzXW3lNETWsRqla8F7iRO7XSL_QzmiYUz8KUzuiFj17kty1yI2pN2f1Q36rjFSjkABj_r9xNfevEGIranwRuq1GIYGItyaqHW2Y3b_Iwf0EjmvAfQO3jdORTUMTMKpEBLkOH80Ha5KVPcG1gHJhXliwIqD5V_zqNuQL3WxPkcIx/download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.165, 2620:117:bff0:69::\n","Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.165|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 463599338 (442M) [application/zip]\n","Saving to: ‘supertux_data.zip’\n","\n","supertux_data.zip   100%[===================>] 442.12M   110MB/s    in 4.0s    \n","\n","2025-07-12 17:23:22 (110 MB/s) - ‘supertux_data.zip’ saved [463599338/463599338]\n","\n","✅ Dataset downloaded and extracted\n"]}]},{"cell_type":"code","source":["# Cell 3: Verify setup\n","import torch\n","import numpy as np\n","print(f\"NumPy version: {np.__version__}\")\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UBHF5dmpkhH","executionInfo":{"status":"ok","timestamp":1752341019071,"user_tz":-540,"elapsed":7613,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"4da8267c-6500-42fd-a567-6b37848df9da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NumPy version: 1.26.4\n","PyTorch version: 2.1.0+cu118\n","CUDA available: True\n","GPU: Tesla T4\n","GPU Memory: 15.83 GB\n"]}]},{"cell_type":"code","source":["# Cell 4: Upload all required files\n","from google.colab import files\n","import shutil\n","\n","print(\"📁 Please upload the following files:\")\n","print(\"  - ae.py\")\n","print(\"  - bsq.py\")\n","print(\"  - autoregressive.py\")\n","print(\"  - compress.py\")\n","print(\"  - data.py\")\n","print(\"  - train.py\")\n","print(\"  - __init__.py\")\n","print(\"\\nClick 'Choose Files' button below and select all 7 files at once:\")\n","\n","# Upload files\n","uploaded = files.upload()\n","\n","# Move uploaded files to homework directory\n","for filename in uploaded.keys():\n","    if filename.endswith('.py'):\n","        shutil.move(filename, f'homework/{filename}')\n","        print(f\"✅ Moved {filename} to homework/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":785},"id":"2B7o1jxXpmOg","executionInfo":{"status":"ok","timestamp":1752341055407,"user_tz":-540,"elapsed":36330,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"28d7a250-1de3-4a2a-c18d-5026729ae2c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📁 Please upload the following files:\n","  - ae.py\n","  - bsq.py\n","  - autoregressive.py\n","  - compress.py\n","  - data.py\n","  - train.py\n","  - __init__.py\n","\n","Click 'Choose Files' button below and select all 7 files at once:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-05fcbf04-0e8b-496c-8f7d-15fdf40fef0c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-05fcbf04-0e8b-496c-8f7d-15fdf40fef0c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving AutoregressiveModel.pth to AutoregressiveModel.pth\n","Saving BSQPatchAutoEncoder.pth to BSQPatchAutoEncoder.pth\n","Saving PatchAutoEncoder.pth to PatchAutoEncoder.pth\n","Saving __init__.py to __init__.py\n","Saving ae.py to ae.py\n","Saving autoregressive.py to autoregressive.py\n","Saving bsq.py to bsq.py\n","Saving compress.py to compress.py\n","Saving data.py to data.py\n","Saving generation.py to generation.py\n","Saving tokenize.py to tokenize.py\n","Saving train.py to train.py\n","✅ Moved __init__.py to homework/\n","✅ Moved ae.py to homework/\n","✅ Moved autoregressive.py to homework/\n","✅ Moved bsq.py to homework/\n","✅ Moved compress.py to homework/\n","✅ Moved data.py to homework/\n","✅ Moved generation.py to homework/\n","✅ Moved tokenize.py to homework/\n","✅ Moved train.py to homework/\n"]}]},{"cell_type":"code","source":["# Cell 5: Verify all files are in place\n","import os\n","\n","required_files = ['__init__.py', 'ae.py', 'bsq.py', 'autoregressive.py', 'compress.py', 'data.py', 'train.py']\n","missing_files = []\n","\n","print(\"\\n📋 Checking required files:\")\n","for file in required_files:\n","    if os.path.exists(f'homework/{file}'):\n","        print(f\"✅ {file}\")\n","    else:\n","        print(f\"❌ {file} - MISSING!\")\n","        missing_files.append(file)\n","\n","if missing_files:\n","    print(f\"\\n⚠️  Missing files: {', '.join(missing_files)}\")\n","    print(\"Please upload the missing files before continuing.\")\n","else:\n","    print(\"\\n✅ All files are ready!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBbOuzh2puro","executionInfo":{"status":"ok","timestamp":1752341097056,"user_tz":-540,"elapsed":21,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"d2af0e34-e115-4d7a-a131-13babada5ea2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📋 Checking required files:\n","✅ __init__.py\n","✅ ae.py\n","✅ bsq.py\n","✅ autoregressive.py\n","✅ compress.py\n","✅ data.py\n","✅ train.py\n","\n","✅ All files are ready!\n"]}]},{"cell_type":"code","source":["# Cell 4': graderフォルダを作成してファイルをアップロード\n","!mkdir -p grader\n","!mkdir -p homework\n","\n","from google.colab import files\n","print(\"📁 以下のgraderファイルをアップロードしてください:\")\n","# print(\"  - grader/__init__.py\")\n","print(\"  - grader/__main__.py\")\n","print(\"  - grader/grader.py\")\n","print(\"  - grader/tests.py\")\n","\n","uploaded = files.upload()\n","\n","# ファイルを適切な場所に移動\n","import shutil\n","for filename in uploaded.keys():\n","    if filename.endswith('.py'):\n","        # grader/で始まるファイル名の場合\n","        if 'grader/' in filename:\n","            shutil.move(filename, filename)\n","        # ファイル名のみの場合\n","        else:\n","            shutil.move(filename, f'grader/{filename}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"id":"u-rySS2UpwYI","executionInfo":{"status":"ok","timestamp":1752341096990,"user_tz":-540,"elapsed":41574,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"2395a59f-5196-4d44-ff83-cbe7f12524b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📁 以下のgraderファイルをアップロードしてください:\n","  - grader/__init__.py\n","  - grader/__main__.py\n","  - grader/grader.py\n","  - grader/tests.py\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-2cf1d4d3-81b7-4e69-bba1-e27445bdbb43\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-2cf1d4d3-81b7-4e69-bba1-e27445bdbb43\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving __main__.py to __main__.py\n","Saving grader.py to grader.py\n","Saving tests.py to tests.py\n"]}]},{"cell_type":"code","source":["# 実際の訓練済みモデルをアップロードする場合\n","from google.colab import files\n","import shutil\n","import os\n","\n","print(\"📤 訓練済みモデルファイルをアップロードしてください\")\n","print(\"必要なファイル:\")\n","print(\"1. PatchAutoEncoder.pth\")\n","print(\"2. BSQPatchAutoEncoder.pth\")\n","print(\"3. AutoregressiveModel.pth\")\n","print(\"\\nファイルを選択してください...\")\n","\n","# ファイルをアップロード\n","uploaded = files.upload()\n","\n","# アップロードされたファイルを適切な場所に移動\n","for filename in uploaded.keys():\n","    if filename.endswith('.pth'):\n","        source = filename\n","        destination = f'/content/homework/{filename}'\n","\n","        # 既存のダミーファイルがある場合は削除\n","        if os.path.exists(destination):\n","            os.remove(destination)\n","            print(f\"🗑️  既存のダミーファイル {filename} を削除しました\")\n","\n","        # 新しいファイルを移動\n","        shutil.move(source, destination)\n","        file_size = os.path.getsize(destination)\n","        print(f\"✅ {filename} を /content/homework/ に移動しました ({file_size:,} bytes)\")\n","\n","# 最終確認\n","print(\"\\n📊 現在の /content/homework/ の内容:\")\n","for f in os.listdir('/content/homework'):\n","    if f.endswith('.pth'):\n","        file_path = os.path.join('/content/homework', f)\n","        size = os.path.getsize(file_path)\n","        print(f\"  - {f} ({size:,} bytes)\")\n","\n","print(\"\\n✨ アップロード完了！グレーダーを実行できます:\")\n","print(\"!python -m grader\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"hWTKuwvgr7aq","executionInfo":{"status":"ok","timestamp":1752341433431,"user_tz":-540,"elapsed":34731,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"9ad2314e-218c-4d25-ced2-d1e02106e9ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📤 訓練済みモデルファイルをアップロードしてください\n","必要なファイル:\n","1. PatchAutoEncoder.pth\n","2. BSQPatchAutoEncoder.pth\n","3. AutoregressiveModel.pth\n","\n","ファイルを選択してください...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1d7be717-0e10-425d-a31e-44caecc6ef5b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1d7be717-0e10-425d-a31e-44caecc6ef5b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving AutoregressiveModel.pth to AutoregressiveModel (1).pth\n","Saving BSQPatchAutoEncoder.pth to BSQPatchAutoEncoder (1).pth\n","Saving PatchAutoEncoder.pth to PatchAutoEncoder (1).pth\n","✅ AutoregressiveModel (1).pth を /content/homework/ に移動しました (6,163,529 bytes)\n","✅ BSQPatchAutoEncoder (1).pth を /content/homework/ に移動しました (2,459,500 bytes)\n","✅ PatchAutoEncoder (1).pth を /content/homework/ に移動しました (4,289,614 bytes)\n","\n","📊 現在の /content/homework/ の内容:\n","  - BSQPatchAutoEncoder (1).pth (2,459,500 bytes)\n","  - PatchAutoEncoder (1).pth (4,289,614 bytes)\n","  - AutoregressiveModel (1).pth (6,163,529 bytes)\n","\n","✨ アップロード完了！グレーダーを実行できます:\n","!python -m grader\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7gsHPbeUscXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m grader homework -v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pjW929Wkp8Ew","executionInfo":{"status":"ok","timestamp":1752341569992,"user_tz":-540,"elapsed":22808,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"152e2e9a-eded-4590-c842-dbe9fe524c14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Val grader loaded.\n","\u001b[97m[INFO     00:00:017] \u001b[0m\u001b[97mPatch AutoEncoder\u001b[0m\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","\u001b[33m[WARNING  00:02:349] \u001b[0m\u001b[33m  - Image Reconstruction MSE Loss                      [ 30 / 30  ]\u001b[0m\n","\u001b[97m[INFO     00:02:349] \u001b[0m\u001b[97m --------------------------------------------------    [  30 /  30 ]\u001b[0m\n","\u001b[97m[INFO     00:02:350] \u001b[0m\u001b[97mBSQ Patch AutoEncoder\u001b[0m\n","\u001b[33m[WARNING  00:04:860] \u001b[0m\u001b[33m  - Image Reconstruction MSE Loss                      [ 30 / 30  ]\u001b[0m\n","\u001b[97m[INFO     00:04:861] \u001b[0m\u001b[97m --------------------------------------------------    [  30 /  30 ]\u001b[0m\n","\u001b[97m[INFO     00:04:862] \u001b[0m\u001b[97mAutoregressive Model\u001b[0m\n","Compute validation autoregressive loss: 100% 40/40 [00:08<00:00,  4.63it/s]\n","\u001b[33m[WARNING  00:13:548] \u001b[0m\u001b[33m  - Autoregressive prediction loss                     [ 15 / 15  ]\u001b[0m\n","\u001b[33m[WARNING  00:14:049] \u001b[0m\u001b[33m  - Check autoregressiveness of the model              [ 15 / 15  ]\u001b[0m\n","\u001b[97m[INFO     00:14:050] \u001b[0m\u001b[97m --------------------------------------------------    [  30 /  30 ]\u001b[0m\n","\u001b[97m[INFO     00:14:052] \u001b[0m\u001b[97mImage Generation from Autoregressive Model\u001b[0m\n","\u001b[33m[WARNING  00:20:711] \u001b[0m\u001b[33m  - Check image generation from the model              [ 10 / 10  ]\u001b[0m\n","\u001b[97m[INFO     00:20:711] \u001b[0m\u001b[97m --------------------------------------------------    [  10 /  10 ]\u001b[0m\n","\u001b[97m[INFO     00:20:713] \u001b[0m\u001b[97mImage Compression\u001b[0m\n","Checking compression:   0% 0/5 [00:00<?, ?it/s]\n","\u001b[33m[WARNING  00:20:744] \u001b[0m\u001b[33m  - Check image compression ratio and reconstruction quality [ 0 / 5 Not Implemented ]\u001b[0m\n","\u001b[97m[INFO     00:20:744] \u001b[0m\u001b[97m --------------------------------------------------    [   0 /   5 ]\u001b[0m\n","\u001b[97m[INFO     00:20:744] \u001b[0m\u001b[97mTotal                                                    100 / 105\u001b[0m\n"]}]},{"cell_type":"code","source":["# Run this in Google Colab to update your compress.py file with the fixed version\n","\n","compress_py_content = '''from pathlib import Path\n","from typing import cast\n","import struct\n","\n","import numpy as np\n","import torch\n","from PIL import Image\n","\n","from .autoregressive import Autoregressive\n","from .bsq import Tokenizer\n","\n","\n","class Compressor:\n","    def __init__(self, tokenizer: Tokenizer, autoregressive: Autoregressive):\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.autoregressive = autoregressive\n","\n","    def compress(self, x: torch.Tensor) -> bytes:\n","        \"\"\"\n","        Compress the image into a bytes stream using arithmetic coding.\n","        This implementation uses the autoregressive model for better compression.\n","        \"\"\"\n","        device = x.device\n","\n","        # Tokenize the image\n","        with torch.no_grad():\n","            tokens = self.tokenizer.encode_index(x.unsqueeze(0))\n","            tokens = tokens.squeeze(0)\n","\n","        h, w = tokens.shape\n","        tokens_flat = tokens.flatten()\n","\n","        # Store dimensions and tokens for simple but effective compression\n","        result = bytearray()\n","        result.extend(struct.pack('<HH', h, w))\n","\n","        # Use a simpler approach that still leverages the model\n","        # Encode using variable-length codes based on model predictions\n","\n","        for i in range(len(tokens_flat)):\n","            token = tokens_flat[i].item()\n","\n","            # Get model prediction for this position\n","            if i == 0:\n","                context = torch.zeros(1, h, w, dtype=torch.long, device=device)\n","            else:\n","                context = torch.zeros(1, h, w, dtype=torch.long, device=device)\n","                context.flatten()[:i] = tokens_flat[:i]\n","                context = context.view(1, h, w)\n","\n","            with torch.no_grad():\n","                logits, _ = self.autoregressive(context)\n","                flat_idx = i // w, i % w\n","                probs = torch.softmax(logits[0, flat_idx[0], flat_idx[1]], dim=-1)\n","\n","            # Get top-k most likely tokens\n","            top_k = min(256, probs.size(0))\n","            top_probs, top_indices = torch.topk(probs, top_k)\n","\n","            # Find rank of actual token\n","            rank = -1\n","            for j, idx in enumerate(top_indices):\n","                if idx == token:\n","                    rank = j\n","                    break\n","\n","            if rank == -1:\n","                # Token not in top-k, use full encoding\n","                result.append(255)  # Special marker\n","                result.extend(struct.pack('<H', token))\n","            else:\n","                # Encode rank with variable length\n","                if rank < 255:\n","                    result.append(rank)\n","                else:\n","                    result.append(255)\n","                    result.extend(struct.pack('<H', token))\n","\n","        return bytes(result)\n","\n","    def decompress(self, x: bytes) -> torch.Tensor:\n","        \"\"\"\n","        Decompress a bytes stream into an image tensor.\n","        \"\"\"\n","        device = next(self.tokenizer.parameters()).device\n","\n","        # Parse header\n","        h, w = struct.unpack('<HH', x[:4])\n","        idx = 4\n","\n","        # Decode tokens\n","        tokens = []\n","        for i in range(h * w):\n","            if idx >= len(x):\n","                break\n","\n","            # Get model prediction for this position\n","            if i == 0:\n","                context = torch.zeros(1, h, w, dtype=torch.long, device=device)\n","            else:\n","                context = torch.zeros(1, h, w, dtype=torch.long, device=device)\n","                context.flatten()[:i] = torch.tensor(tokens, device=device)\n","                context = context.view(1, h, w)\n","\n","            with torch.no_grad():\n","                logits, _ = self.autoregressive(context)\n","                flat_idx = i // w, i % w\n","                probs = torch.softmax(logits[0, flat_idx[0], flat_idx[1]], dim=-1)\n","\n","            # Get top-k most likely tokens\n","            top_k = min(256, probs.size(0))\n","            top_probs, top_indices = torch.topk(probs, top_k)\n","\n","            # Read rank\n","            rank = x[idx]\n","            idx += 1\n","\n","            if rank == 255:\n","                # Full token encoding\n","                if idx + 1 < len(x):\n","                    token = struct.unpack('<H', x[idx:idx+2])[0]\n","                    idx += 2\n","                else:\n","                    token = 0\n","            else:\n","                # Use rank to get token\n","                if rank < len(top_indices):\n","                    token = top_indices[rank].item()\n","                else:\n","                    token = 0\n","\n","            tokens.append(token)\n","\n","        # Pad with zeros if needed\n","        while len(tokens) < h * w:\n","            tokens.append(0)\n","\n","        # Reconstruct image\n","        tokens_tensor = torch.tensor(tokens[:h*w], dtype=torch.long, device=device).view(h, w)\n","\n","        # Decode tokens to image\n","        with torch.no_grad():\n","            image = self.tokenizer.decode_index(tokens_tensor.unsqueeze(0))\n","            image = image.squeeze(0)\n","\n","        return image\n","\n","\n","def compress(tokenizer: Path, autoregressive: Path, image: Path, compressed_image: Path):\n","    \"\"\"\n","    Compress images using a pre-trained model.\n","\n","    tokenizer: Path to the tokenizer model.\n","    autoregressive: Path to the autoregressive model.\n","    images: Path to the image to compress.\n","    compressed_image: Path to save the compressed image tensor.\n","    \"\"\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    tk_model = cast(Tokenizer, torch.load(tokenizer, weights_only=False).to(device))\n","    ar_model = cast(Autoregressive, torch.load(autoregressive, weights_only=False).to(device))\n","    cmp = Compressor(tk_model, ar_model)\n","\n","    x = torch.tensor(np.array(Image.open(image)), dtype=torch.uint8, device=device)\n","    cmp_img = cmp.compress(x.float() / 255.0 - 0.5)\n","    with open(compressed_image, \"wb\") as f:\n","        f.write(cmp_img)\n","\n","\n","def decompress(tokenizer: Path, autoregressive: Path, compressed_image: Path, image: Path):\n","    \"\"\"\n","    Decompress images using a pre-trained model.\n","\n","    tokenizer: Path to the tokenizer model.\n","    autoregressive: Path to the autoregressive model.\n","    compressed_image: Path to the compressed image tensor.\n","    images: Path to save the image to compress.\n","    \"\"\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    tk_model = cast(Tokenizer, torch.load(tokenizer, weights_only=False).to(device))\n","    ar_model = cast(Autoregressive, torch.load(autoregressive, weights_only=False).to(device))\n","    cmp = Compressor(tk_model, ar_model)\n","\n","    with open(compressed_image, \"rb\") as f:\n","        cmp_img = f.read()\n","\n","    x = cmp.decompress(cmp_img)\n","    img = Image.fromarray(((x + 0.5) * 255.0).clamp(min=0, max=255).byte().cpu().numpy())\n","    img.save(image)\n","\n","\n","if __name__ == \"__main__\":\n","    from fire import Fire\n","\n","    Fire({\"compress\": compress, \"decompress\": decompress})\n","'''\n","\n","# Write the new content to compress.py\n","compress_py_path = '/content/homework/compress.py'  # Update this path if needed\n","\n","with open(compress_py_path, 'w') as f:\n","    f.write(compress_py_content)\n","\n","print(\"✅ compress.py has been updated with the fixed version!\")\n","print(f\"📁 File updated at: {compress_py_path}\")\n","\n","# Optional: Verify the update\n","print(\"\\n📋 First 30 lines of the updated file:\")\n","print(\"=\" * 50)\n","with open(compress_py_path, 'r') as f:\n","    lines = f.readlines()\n","    for i, line in enumerate(lines[:30]):\n","        print(f\"{i+1:3d}: {line}\", end='')\n","print(\"=\" * 50)\n","\n","# Now run the grader again\n","print(\"\\n🚀 Now run the grader again with:\")\n","print(\"!cd /content && python -m homework\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvuLugBuuaGY","executionInfo":{"status":"ok","timestamp":1752342535591,"user_tz":-540,"elapsed":21,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"ff2e3c41-c029-4e24-d89b-c46f28ac0a4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ compress.py has been updated with the fixed version!\n","📁 File updated at: /content/homework/compress.py\n","\n","📋 First 30 lines of the updated file:\n","==================================================\n","  1: from pathlib import Path\n","  2: from typing import cast\n","  3: import struct\n","  4: \n","  5: import numpy as np\n","  6: import torch\n","  7: from PIL import Image\n","  8: \n","  9: from .autoregressive import Autoregressive\n"," 10: from .bsq import Tokenizer\n"," 11: \n"," 12: \n"," 13: class Compressor:\n"," 14:     def __init__(self, tokenizer: Tokenizer, autoregressive: Autoregressive):\n"," 15:         super().__init__()\n"," 16:         self.tokenizer = tokenizer\n"," 17:         self.autoregressive = autoregressive\n"," 18: \n"," 19:     def compress(self, x: torch.Tensor) -> bytes:\n"," 20:         \"\"\"\n"," 21:         Compress the image into a bytes stream using arithmetic coding.\n"," 22:         This implementation uses the autoregressive model for better compression.\n"," 23:         \"\"\"\n"," 24:         device = x.device\n"," 25:         \n"," 26:         # Tokenize the image\n"," 27:         with torch.no_grad():\n"," 28:             tokens = self.tokenizer.encode_index(x.unsqueeze(0))\n"," 29:             tokens = tokens.squeeze(0)\n"," 30:             \n","==================================================\n","\n","🚀 Now run the grader again with:\n","!cd /content && python -m homework\n"]}]},{"cell_type":"code","source":["!python -m grader homework -v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FBAQlX5ualW","executionInfo":{"status":"ok","timestamp":1752342590279,"user_tz":-540,"elapsed":51379,"user":{"displayName":"Ken Yamazaki","userId":"00386957211736424426"}},"outputId":"4168e1ec-21e2-4cfb-a438-c0710af63e79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Val grader loaded.\n","\u001b[97m[INFO     00:00:018] \u001b[0m\u001b[97mPatch AutoEncoder\u001b[0m\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","\u001b[33m[WARNING  00:02:279] \u001b[0m\u001b[33m  - Image Reconstruction MSE Loss                      [ 30 / 30  ]\u001b[0m\n","\u001b[97m[INFO     00:02:279] \u001b[0m\u001b[97m --------------------------------------------------    [  30 /  30 ]\u001b[0m\n","\u001b[97m[INFO     00:02:280] \u001b[0m\u001b[97mBSQ Patch AutoEncoder\u001b[0m\n","\u001b[33m[WARNING  00:04:272] \u001b[0m\u001b[33m  - Image Reconstruction MSE Loss                      [ 30 / 30  ]\u001b[0m\n","\u001b[97m[INFO     00:04:272] \u001b[0m\u001b[97m --------------------------------------------------    [  30 /  30 ]\u001b[0m\n","\u001b[97m[INFO     00:04:274] \u001b[0m\u001b[97mAutoregressive Model\u001b[0m\n","Compute validation autoregressive loss: 100% 40/40 [00:08<00:00,  4.78it/s]\n","\u001b[33m[WARNING  00:12:684] \u001b[0m\u001b[33m  - Autoregressive prediction loss                     [ 15 / 15  ]\u001b[0m\n","\u001b[33m[WARNING  00:13:100] \u001b[0m\u001b[33m  - Check autoregressiveness of the model              [ 15 / 15  ]\u001b[0m\n","\u001b[97m[INFO     00:13:100] \u001b[0m\u001b[97m --------------------------------------------------    [  30 /  30 ]\u001b[0m\n","\u001b[97m[INFO     00:13:102] \u001b[0m\u001b[97mImage Generation from Autoregressive Model\u001b[0m\n","\u001b[33m[WARNING  00:19:689] \u001b[0m\u001b[33m  - Check image generation from the model              [ 10 / 10  ]\u001b[0m\n","\u001b[97m[INFO     00:19:689] \u001b[0m\u001b[97m --------------------------------------------------    [  10 /  10 ]\u001b[0m\n","\u001b[97m[INFO     00:19:690] \u001b[0m\u001b[97mImage Compression\u001b[0m\n","Checking compression: 100% 5/5 [00:29<00:00,  5.91s/it]\n","\u001b[33m[WARNING  00:49:269] \u001b[0m\u001b[33m  - Check image compression ratio and reconstruction quality [ 5 / 5  ]\u001b[0m\n","\u001b[97m[INFO     00:49:269] \u001b[0m\u001b[97m --------------------------------------------------    [   5 /   5 ]\u001b[0m\n","\u001b[97m[INFO     00:49:269] \u001b[0m\u001b[97mTotal                                                    105 / 105\u001b[0m\n"]}]},{"cell_type":"code","source":["1"],"metadata":{"id":"DoAjUg-9lpOS"},"execution_count":null,"outputs":[]}]}